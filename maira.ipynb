{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:06<00:00,  1.13s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2974\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2972\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2973\u001b[0m         )\n\u001b[0;32m-> 2974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/gpfs3/well/papiez/users/hri611/python/foundation-models-radiology/maira_env/lib/python3.10/site-packages/torch/cuda/__init__.py:372\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    371\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    376\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "local_model_path = Path(\"maira-2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.eval()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('/well/papiez/users/hri611/python/foundation-models-radiology')\n",
    "\n",
    "jpg_paths = glob.glob(str(root_dir / 'PTX Head to Head Study Data' / '**/*.jpg'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▏                                                                  | 45/413 [07:31<1:18:20, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image /well/papiez/users/hri611/python/foundation-models-radiology/PTX Head to Head Study Data/641c8fb92052b371c8ae07739b3e3165/8085bd0484a67696e4ea94210be0af57/0d485bfdf8efe60bce38fa2e0512f1bc/7674a6468f2b10c10dba634e1d5a3524.jpg: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████▎                                                      | 116/413 [18:06<41:09,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image /well/papiez/users/hri611/python/foundation-models-radiology/PTX Head to Head Study Data/11739350140b85317fa7e1583f1c8b05/5c24a3bca2a5c1e00df087ace2743b37/e9224f78ba18d6eb80121bd49b5df0c6/7eb5a9238462b0910dac0a400301c7e1.jpg: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████▏                    | 300/413 [48:29<24:02, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image /well/papiez/users/hri611/python/foundation-models-radiology/PTX Head to Head Study Data/2aa4e51b9bbd537ee166b9b02716ab21/085e8ec77cac84257529ba7796ce552b/5cafb839b2b0a2c8c9fcc9e64e2857a2/c6a0076ae7bdc9184285740b16573c96.jpg: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 413/413 [1:06:49<00:00,  9.71s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "grounded_predictions = []\n",
    "\n",
    "for image_path in tqdm(jpg_paths):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # non-grounded report\n",
    "    processed_inputs = processor.format_and_preprocess_reporting_input(\n",
    "        current_frontal=img,\n",
    "        current_lateral=None,\n",
    "        prior_frontal=None,  # Our example has no prior\n",
    "        indication=None,\n",
    "        technique='PA view of the chest',\n",
    "        comparison=None,\n",
    "        prior_report=None,  # Our example has no prior\n",
    "        return_tensors=\"pt\",\n",
    "        get_grounding=False,  # For this example we generate a non-grounded report\n",
    "    )\n",
    "\n",
    "    processed_inputs = processed_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_decoding = model.generate(\n",
    "            **processed_inputs,\n",
    "            max_new_tokens=300,  # Set to 450 for grounded reporting\n",
    "            use_cache=True,\n",
    "        )\n",
    "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
    "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
    "    decoded_text = decoded_text.lstrip()  # Findings generation completions have a single leading space\n",
    "    prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
    "\n",
    "    # grounded report\n",
    "    processed_inputs = processor.format_and_preprocess_phrase_grounding_input(\n",
    "        frontal_image= img,\n",
    "        phrase='Pneumothorax',\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    processed_inputs = processed_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_decoding = model.generate(\n",
    "            **processed_inputs,\n",
    "            max_new_tokens=150,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
    "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "    try: # some errors for images because of decoded_text output\n",
    "        grounded_prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
    "\n",
    "        # adjust bbox coordinates for image size\n",
    "        for i in range(len(grounded_prediction)):\n",
    "            if grounded_prediction[i][1] is None:\n",
    "                continue\n",
    "            else:\n",
    "                coords = grounded_prediction[i][1][0]\n",
    "                adjusted_coords = processor.adjust_box_for_original_image_size(coords, width = img.size[0], height = img.size[1])\n",
    "                grounded_prediction[i][1][0] = adjusted_coords\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        grounded_prediction = None\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    grounded_predictions.append(grounded_prediction)\n",
    "\n",
    "maira_results_df = pd.DataFrame()\n",
    "maira_results_df['image_path'] = jpg_paths\n",
    "maira_results_df['prediction'] = predictions\n",
    "maira_results_df['grounded_prediction'] = grounded_predictions\n",
    "\n",
    "maira_results_df['image_path'] = maira_results_df['image_path'].apply(lambda x: x.replace('/well/papiez/users/hri611/python/foundation-models-radiology/PTX Head to Head Study Data/', ''))\n",
    "\n",
    "maira_results_df.to_csv('ptx_maira_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('/well/papiez/users/hri611/python/foundation-models-radiology')\n",
    "\n",
    "dicom_paths = glob.glob(str(root_dir / 'H2H Study - OUH Cases.nosync' / '**/*.jpg'), recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "grounded_predictions = []\n",
    "\n",
    "for image_path in tqdm(jpg_paths):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # non-grounded report\n",
    "    processed_inputs = processor.format_and_preprocess_reporting_input(\n",
    "        current_frontal=img,\n",
    "        current_lateral=None,\n",
    "        prior_frontal=None,  # Our example has no prior\n",
    "        indication=None,\n",
    "        technique='X-ray',\n",
    "        comparison=None,\n",
    "        prior_report=None,  # Our example has no prior\n",
    "        return_tensors=\"pt\",\n",
    "        get_grounding=False,  # For this example we generate a non-grounded report\n",
    "    )\n",
    "\n",
    "    processed_inputs = processed_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_decoding = model.generate(\n",
    "            **processed_inputs,\n",
    "            max_new_tokens=300,  # Set to 450 for grounded reporting\n",
    "            use_cache=True,\n",
    "        )\n",
    "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
    "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
    "    decoded_text = decoded_text.lstrip()  # Findings generation completions have a single leading space\n",
    "    prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
    "\n",
    "    # grounded report\n",
    "    processed_inputs = processor.format_and_preprocess_phrase_grounding_input(\n",
    "        frontal_image= img,\n",
    "        phrase='Fracture',\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    processed_inputs = processed_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_decoding = model.generate(\n",
    "            **processed_inputs,\n",
    "            max_new_tokens=150,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
    "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "    try: # some errors for images because of decoded_text output\n",
    "        grounded_prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
    "\n",
    "        # adjust bbox coordinates for image size\n",
    "        for i in range(len(grounded_prediction)):\n",
    "            if grounded_prediction[i][1] is None:\n",
    "                continue\n",
    "            else:\n",
    "                coords = grounded_prediction[i][1][0]\n",
    "                adjusted_coords = processor.adjust_box_for_original_image_size(coords, width = img.size[0], height = img.size[1])\n",
    "                grounded_prediction[i][1][0] = adjusted_coords\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        grounded_prediction = None\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    grounded_predictions.append(grounded_prediction)\n",
    "\n",
    "maira_results_df = pd.DataFrame()\n",
    "maira_results_df['image_path'] = jpg_paths\n",
    "maira_results_df['prediction'] = predictions\n",
    "maira_results_df['grounded_prediction'] = grounded_predictions\n",
    "\n",
    "maira_results_df['image_path'] = maira_results_df['image_path'].apply(lambda x: x.replace('/well/papiez/users/hri611/python/foundation-models-radiology/PTX Head to Head Study Data/', ''))\n",
    "\n",
    "maira_results_df.to_csv('ftx_maira_scores.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
