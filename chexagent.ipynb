{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set both for redundancy\n",
    "os.environ[\"HF_HOME\"] = \"/gpfs3/well/papiez/users/hri611/.cache/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/gpfs3/well/papiez/users/hri611/.cache/huggingfacee\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/gpfs3/well/papiez/users/hri611/.cache/huggingface/datasets\"\n",
    "os.environ[\"HF_METRICS_CACHE\"] = \"/gpfs3/well/papiez/users/hri611/.cache/huggingface/metrics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, AutoTokenizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Setup constant\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "# step 2: Load Processor and Model\n",
    "processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\n",
    "generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StanfordAIMI/CheXagent-8b\", torch_dtype=dtype, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('/well/papiez/users/hri611/python/foundation-models-radiology')\n",
    "dicom_paths = glob.glob(str(root_dir / 'PTX Head to Head Study Data' / '**/*.dcm'), recursive=True)\n",
    "\n",
    "def dicom_to_rgb_image(dicom_path):\n",
    "    dicom_image = pydicom.dcmread(dicom_path)\n",
    "    pixel_array = dicom_image.pixel_array\n",
    "\n",
    "    # Normalize to 0-255\n",
    "    pixel_array = (pixel_array - np.min(pixel_array)) / (np.max(pixel_array) - np.min(pixel_array) + 1e-5)\n",
    "    pixel_array = (pixel_array * 255.0).astype(np.uint8)\n",
    "\n",
    "    # Convert grayscale to RGB\n",
    "    if pixel_array.ndim == 2:\n",
    "        image = Image.fromarray(pixel_array).convert(\"RGB\")\n",
    "    elif pixel_array.ndim == 3 and pixel_array.shape[-1] == 3:\n",
    "        image = Image.fromarray(pixel_array)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected DICOM shape: {pixel_array.shape}\")\n",
    "    \n",
    "    # save as jpg\n",
    "    jpg_path = dicom_path.replace('.dcm', '.jpg')\n",
    "    image.save(jpg_path, \"JPEG\")\n",
    "\n",
    "# images = [dicom_to_rgb_image(dicom_paths[0])]\n",
    "for dicom in dicom_paths:\n",
    "    dicom_to_rgb_image(dicom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8B Model\n",
    "# # step 3: Fetch the images\n",
    "# # image_path = 'Pleural_effusion-Metastatic_breast_carcinoma_Case_166_(5477628658).jpg'\n",
    "# # images = [Image.open(image_path).convert(\"RGB\")]\n",
    "\n",
    "# # step 4: Generate the Findings section\n",
    "# dtype = model.dtype\n",
    "# model.to(device)\n",
    "\n",
    "# prompt = f'Does this chest X-ray contain a pneumothorax?'\n",
    "# inputs = processor(images=images, text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(device=device, dtype=dtype)\n",
    "# output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "# response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Setup constant\n",
    "#bfloat16 (Brain Floating Point) is a 16-bit format optimized for deep learning:\n",
    "#Faster and more memory-efficient than float32\n",
    "model_name = \"StanfordAIMI/CheXagent-2-3b\"\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "\n",
    "# step 2: Load Processor and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "model = model.to(device=device,dtype=dtype)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for i in range(len(jpg_paths)):  \n",
    "    query = tokenizer.from_list_format([{'image': jpg_paths[i]},{'text': 'Does this chest X-ray contain a pneumothorax?'}])\n",
    "    conv = [{\"from\": \"system\", \"value\": \"You are a helpful assistant.\"}, {\"from\": \"human\", \"value\": query}]\n",
    "    input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids.to(device), do_sample=False, num_beams=1, temperature=1., top_p=1., use_cache=True,\n",
    "        max_new_tokens=512\n",
    "    )[0]\n",
    "    response = tokenizer.decode(output[input_ids.size(1):-1])\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexagent_df = pd.DataFrame({'image_path': jpg_paths, 'response': responses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexagent_df['binary_score'] = chexagent_df['response'].apply(lambda x: 1 if 'yes' in x.lower() else 0)\n",
    "chexagent_df.drop(columns=['response'], inplace=True)\n",
    "chexagent_df.to_csv('ptx_chexagent_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_df = pd.read_csv('ptx_biomedclip_scores.csv')\n",
    "google_df = pd.read_csv('ptx_google_scores.csv')\n",
    "\n",
    "merged_df = pd.merge(microsoft_df, google_df, on='image_paths', suffixes=('_microsoft', '_google'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['chexagent_scores'] = chexagent_df['binary_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['binary_score_google','binary_score_microsoft','chexagent_scores']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
