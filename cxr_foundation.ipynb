{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anissa218/foundation-models-radiology/blob/main/notebooks/quick_start_with_hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjF6yQryA7mo"
   },
   "source": [
    "~~~\n",
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "~~~\n",
    "<table><tbody><tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/google-health/cxr-foundation/blob/master/notebooks/quick_start_with_hugging_face.ipynb\">\n",
    "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/google-health/cxr-foundation/blob/master/notebooks/quick_start_with_hugging_face.ipynb\">\n",
    "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://huggingface.co/google/cxr-foundation\">\n",
    "      <img alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on HuggingFace\n",
    "    </a>\n",
    "  </td>\n",
    "</tr></tbody></table>\n",
    "\n",
    "# Quick start with Hugging Face\n",
    "This Colab notebook provides a basic demo of using Chest X-ray (CXR) Foundation. CXR Foundation is an embeddings models that generates a machine learning representations known as embeddings, from chest X-ray images and/or chest X-ray related text. These embeddings can be used to develop custom models for CXR use-cases with less data and compute compared to traditional model development methods. Learn more about embeddings and their benefits at this [page](https://developers.google.com/health-ai-developer-foundations/cxr-foundation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "m3HdxUsO74d7",
    "outputId": "03736976-11d2-4246-fef3-be131e911d27"
   },
   "outputs": [],
   "source": [
    "#!pip install 'numpy>=1.22,<1.26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "37d69aa059f74443a1b40bdab525a1d3",
      "85b594bab9704d618aeb5f5e8b7455a4",
      "f3fa7de65c3645a281b882604c57ead0",
      "8d33f3e5fc9b4d4582ca8d3068c04fdc",
      "67db1e9b606442b5a0e374126729e378",
      "001414df35244a3b8fdbda60ddc24a26",
      "bfb85cf7d3e24d189436b391f1c5f1e6",
      "e8d3b80382ab49ec92af236abf1a2509",
      "683e4a90c20648c6bca0472c6a5b3e74",
      "55756596ccad4e83bc42a54691369b8d",
      "e2e902deb09b4c9d9fff238751917f39",
      "d599dd1557fe443a8014575cd0921261",
      "c956000cb559431d91fc569d943c5b7c",
      "bc5f7475c42949a297e2410fba56430e",
      "90b7f9d3c6314e6aa8d57ee36b69d57c",
      "048847e5a1514c919017f4f4e9671fc8",
      "a8bbda669fc841558b98375eeb2ae0fb",
      "b2daa7a368294882aec9b31e0f18849f",
      "e41a5fe49c83463c8e955c65cb1e9645",
      "05749aab6b7b450d8982be6abe65e694"
     ]
    },
    "id": "_I07AumeA6Bn",
    "outputId": "38f19dc9-83fd-482f-d3f3-005da5d828db"
   },
   "outputs": [],
   "source": [
    "# @title Authenticate with HuggingFace, skip if you have a HF_TOKEN secret\n",
    "\n",
    "# Authenticate user for HuggingFace if needed. Enter token below if requested.\n",
    "from huggingface_hub.utils import HfFolder\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "# if HfFolder.get_token() is None:\n",
    "#     from huggingface_hub import notebook_login\n",
    "#     notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0eVPU0GbKC_",
    "outputId": "4b6d0f27-1c26-4a3a-8989-273d183c6314"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions to prepare inputs: text & image TF Example\n",
    "#!pip install tensorflow-text==2.17 pypng 2>&1 1>/dev/null\n",
    "import io\n",
    "import png\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import tensorflow_hub as tf_hub\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for tokenizing text input\n",
    "def bert_tokenize(text):\n",
    "    \"\"\"Tokenizes input text and returns token IDs and padding masks.\"\"\"\n",
    "    preprocessor = tf_hub.KerasLayer(\n",
    "        \"/well/papiez/users/hri611/python/foundation-models-radiology/models/tensorflow/bert/tensorFlow2/en-uncased-preprocess/3\")\n",
    "    out = preprocessor(tf.constant([text.lower()]))\n",
    "    ids = out['input_word_ids'].numpy().astype(np.int32)\n",
    "    masks = out['input_mask'].numpy().astype(np.float32)\n",
    "    paddings = 1.0 - masks\n",
    "    end_token_idx = ids == 102\n",
    "    ids[end_token_idx] = 0\n",
    "    paddings[end_token_idx] = 1.0\n",
    "    ids = np.expand_dims(ids, axis=1)\n",
    "    paddings = np.expand_dims(paddings, axis=1)\n",
    "    assert ids.shape == (1, 1, 128)\n",
    "    assert paddings.shape == (1, 1, 128)\n",
    "    return ids, paddings\n",
    "\n",
    "# Helper function for processing image data\n",
    "def png_to_tfexample(image_array: np.ndarray) -> tf.train.Example:\n",
    "    \"\"\"Creates a tf.train.Example from a NumPy array.\"\"\"\n",
    "    # Convert the image to float32 and shift the minimum value to zero\n",
    "    image = image_array.astype(np.float32)\n",
    "    image -= image.min()\n",
    "\n",
    "    if image_array.dtype == np.uint8:\n",
    "        # For uint8 images, no rescaling is needed\n",
    "        pixel_array = image.astype(np.uint8)\n",
    "        bitdepth = 8\n",
    "    else:\n",
    "        # For other data types, scale image to use the full 16-bit range\n",
    "        max_val = image.max()\n",
    "        if max_val > 0:\n",
    "            image *= 65535 / max_val  # Scale to 16-bit range\n",
    "        pixel_array = image.astype(np.uint16)\n",
    "        bitdepth = 16\n",
    "\n",
    "    # Ensure the array is 2-D (grayscale image)\n",
    "    if pixel_array.ndim != 2:\n",
    "        raise ValueError(f'Array must be 2-D. Actual dimensions: {pixel_array.ndim}')\n",
    "\n",
    "    # Encode the array as a PNG image\n",
    "    output = io.BytesIO()\n",
    "    png.Writer(\n",
    "        width=pixel_array.shape[1],\n",
    "        height=pixel_array.shape[0],\n",
    "        greyscale=True,\n",
    "        bitdepth=bitdepth\n",
    "    ).write(output, pixel_array.tolist())\n",
    "    png_bytes = output.getvalue()\n",
    "\n",
    "    # Create a tf.train.Example and assign the features\n",
    "    example = tf.train.Example()\n",
    "    features = example.features.feature\n",
    "    features['image/encoded'].bytes_list.value.append(png_bytes)\n",
    "    features['image/format'].bytes_list.value.append(b'png')\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvRLGnbX8Kv0",
    "outputId": "21e52a48-c1b8-4798-d393-f4cdf8ec22a2"
   },
   "outputs": [],
   "source": [
    "#!pip install pydicom\n",
    "import pydicom\n",
    "\n",
    "def dicom_to_tfexample(dicom_path: str) -> tf.train.Example:\n",
    "    \"\"\"Reads a DICOM image and converts it into a tf.train.Example.\"\"\"\n",
    "\n",
    "    # Load DICOM file\n",
    "    dcm = pydicom.dcmread(dicom_path)\n",
    "    image_array = dcm.pixel_array.astype(np.float32)\n",
    "\n",
    "    # Normalize the image (shift min to 0)\n",
    "    image_array -= image_array.min()\n",
    "\n",
    "    # Determine bit depth\n",
    "    max_val = image_array.max()\n",
    "    if dcm.BitsStored == 8 or max_val <= 255:\n",
    "        pixel_array = image_array.astype(np.uint8)\n",
    "        bitdepth = 8\n",
    "    else:\n",
    "        if max_val > 0:\n",
    "            image_array *= 65535 / max_val  # Scale to 16-bit range\n",
    "        pixel_array = image_array.astype(np.uint16)\n",
    "        bitdepth = 16\n",
    "\n",
    "    # Encode the array as a PNG image\n",
    "    output = io.BytesIO()\n",
    "    png.Writer(\n",
    "        width=pixel_array.shape[1],\n",
    "        height=pixel_array.shape[0],\n",
    "        greyscale=True,\n",
    "        bitdepth=bitdepth\n",
    "    ).write(output, pixel_array.tolist())\n",
    "    png_bytes = output.getvalue()\n",
    "\n",
    "    # Create a tf.train.Example and assign the features\n",
    "    example = tf.train.Example()\n",
    "    features = example.features.feature\n",
    "    features['image/encoded'].bytes_list.value.append(png_bytes)\n",
    "    features['image/format'].bytes_list.value.append(b'png')\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRqJk9D_9fBa"
   },
   "source": [
    "# Generate embeddings from dicom images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYog_nfH9fTV"
   },
   "outputs": [],
   "source": [
    "# load images (first drag into folder)\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "root_dir = Path('/well/papiez/users/hri611/python/foundation-models-radiology')\n",
    "\n",
    "# Find all DICOM files under folders like 017positive/*/*/*/*.dcm and 017negative/*/*/*/*.dcm\n",
    "# dicom_paths = glob.glob(str(root_dir / 'pnx'/ '017Positive/**/*.dcm'), recursive=True)\n",
    "# dicom_paths += glob.glob(str(root_dir / 'pnx' / '017Negative/**/*.dcm'), recursive=True)\n",
    "\n",
    "dicom_paths = glob.glob(str(root_dir / 'PTX Head to Head Study Data' / '**/*.dcm'), recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672,
     "referenced_widgets": [
      "d71b6233604d406a8893cb84a8fc2fb6",
      "0348b328bf814d0599ed142a7f4991d4",
      "fac362e5d42842a39e3ceb101efb2aa4",
      "13321d8a7f97418a8bf902cdd567d3b6",
      "1d32b54291fc41b49a919279a9f34567",
      "8cfa9333c85c4d19b0191de86cf3d324",
      "2731d7fd489343818987237ca8c0bfc8",
      "832ade9105ec4e4781d972a64644a408",
      "f4075c73bb9143409f963cddad60c7c1",
      "2810c4194f9b44d0b758704e851b36a0",
      "0f7a550842a84ada86e697d46bdaec6f",
      "07329132c8b04996a0bb558c3a890eb9",
      "18d66aa32d984ebd8d6fb1cda1149984",
      "46fd3ef48806490c891f621c6b682e00",
      "0452366a82a54a9f944b779501f6362f",
      "c97ac1077b20422ab8a007046e046823",
      "5ebc1cfcbd5649f49cb3582e6607e0f2",
      "16c284cac8d24428ba52227b9e0ed754",
      "6eae7b33e73b4858b14c792b87b9f7e9",
      "f33ed32c993848eb951c495fcf0ea908",
      "09a96e4a2cbd4513b0a1829fb3a6b99e",
      "642b645a76aa427788fc4a390b19fbb8",
      "0b56634cc4d34be49e781d119fb907b0",
      "98d7c80958554017981cd50b67df2175",
      "45d67da1568f4e55b93a4f7e7336af06",
      "3c02c226807a41e09308c1bbfd832c0d",
      "c8e4348f92f641b69e9e159b39fbc658",
      "db5dea1a0f804773bc53c95a2678ce79",
      "bf0c035ddd4a410f9dbf888b74fd710f",
      "a318d29e8b514c1eb68ef347cc56b125",
      "05d7859412a64eb8bad20324cd4fc167",
      "1832c21494274d28a24ba59fe0205f27",
      "c0af22b689d7466bb67621091a211ba8",
      "fb877db921904b4bae5fe9ea5785a3bc",
      "849c63c882cd4546adf984011db0010f",
      "9a99773a95864543b9e2a08497f7a175",
      "76b82069c25d4051b1ea6393aa426deb",
      "5b1dc7c0eaa4461b92739233a692457e",
      "32d1d0a2b91e43dfb8257acfdd8e087d",
      "48d62081800945d4999be3746e3610ad",
      "e27c845aa9ba4b839a908f672a1a383d",
      "6b9afe02d2f74f119cb06b2da55aeeba",
      "66e56b3a89f448a8939b7ff82ca9720a",
      "20e7676bf4c24db095c01dc64151e2d6",
      "5e479804772b4a138160e445f7b879ae",
      "dd1e556049584eee918611be6e66ccfb",
      "11dab0d1ab914ffb879c2a928e870ace",
      "8ffeb9f169834ea29078edaae32fc188",
      "8d14ecbd5c254508905e2bc07fa82451",
      "9d2b6745da8b4e16b4f04c16cbcee581",
      "82267aa067db489893b9b79fa3d18251",
      "5e79c95b39c540efa6579fea9073ebc5",
      "5524af264d1249c78b9d1b8c644a30cc",
      "077e074179b147dc8edcda6b6797d2e3",
      "1bd9ba32322544eba41d8047ff3f0085",
      "e3fa3ec053ec422e8485a6b690287149",
      "c4ef3d1cb8e3428ea7014285fb96c06b",
      "f47b4d3451dc41028ae6eadb3e04a0ff",
      "94335a77d4f24e0ba66fbfb0d39e9842",
      "6d687fa57566431c85100636495e636e",
      "0e8310f359f1465ea5d336bcc7ce253f",
      "233453bb4a58485994e20bed29a93866",
      "de42a32703cc421bb0203e7cde7cc4c1",
      "94c88de45745481ca9a5a918ef04cc4a",
      "2167f188412a475da37a68aac968f896",
      "78004824b2f34f768819293418d1f542",
      "5b2c6d32324444f3b5576e750a538177",
      "b70317628f694801bf693736ade4445c",
      "73f43543f7804ebabb854583b372697e",
      "ea45525169e7423f90d2e2097c858df5",
      "4143789c2d004570975e63ee4463b355",
      "3176f9d05fe0457088edd104bfdf7d01",
      "bb9fa7718a164f6b8e541c793a21a660",
      "72953ba04dc2449aaf6288c358746abb",
      "d76684f544dd43a39176ecc0c01c1707",
      "2cc4d629fa5a4e4d9c55321c6dca2e99",
      "62d1d43728224f6c9dd4ca1c641ecb47",
      "45d1fe9397d84206b7c6c0c278daefec",
      "b084b6d4beb94d3798da1f0fbbb930c4",
      "db25bf6a44db4d5f93a73e3c4d20a049",
      "ca9391cb835a40d0b51f958212d8ef45",
      "3842814e67a14737bca9ab3c395cf9e4",
      "ee342f54dc7a4e0996abb57ae3850de4",
      "70144aa77c4f45c6831a2f05be6966ac",
      "4b6573765bd64258bcdecf703c26c4ff",
      "93d93ca0adc14492bfa4912fe672afc0",
      "73ba14dbd637449c9d9f4910dcf50cac",
      "1a2817bde8404a3f95dd06cb0d25ada4",
      "ca889890341644fdbce092ce03e911a6",
      "f6a63e42c9cd424595cf332b29c094d3",
      "aba24674b6914d4b9887383958007cc2",
      "a85247d0ef9a4d8b911e74894b4154d7",
      "dfd2d0c0b9434e9c80e0885f49170829",
      "1a7981afb9d840b5bbc0854b80766782",
      "fcb442bf0f584b71aaaf865f105b2001",
      "f41f7ffad5d94cf8969c3be9d72bb088",
      "00471dabcd5941efbcd05d1a8be0e27f",
      "c342989b979a4cb29992f4aa0261869b",
      "72610016490c4311880d3c1c95b744ae"
     ]
    },
    "id": "QXPpAoU590Zb",
    "outputId": "20ad1916-c9ce-46eb-da8c-8433cb9935fb"
   },
   "outputs": [],
   "source": [
    "# @title Download model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "root_dir = '/well/papiez/users/hri611/python/foundation-models-radiology'\n",
    "snapshot_download(repo_id=\"google/cxr-foundation\",local_dir=os.path.join(root_dir,'hf'),\n",
    "                  allow_patterns=['elixr-c-v2-pooled/*', 'pax-elixr-b-text/*']) # instead of /content/hf\n",
    "\n",
    "if 'elixrc_model' not in locals():\n",
    "  elixrc_model = tf.saved_model.load(os.path.join(root_dir,'hf','elixr-c-v2-pooled'))\n",
    "  elixrc_infer = elixrc_model.signatures['serving_default']\n",
    "\n",
    "if 'qformer_model' not in locals():\n",
    "  qformer_model = tf.saved_model.load(os.path.join(root_dir,'hf','pax-elixr-b-text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u7_TUll93-m",
    "outputId": "d198f88d-5cda-437b-e211-f946676eb179"
   },
   "outputs": [],
   "source": [
    "# @title Generate image embeddings\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# image_paths = list of image dcm paths - could also try to get true label\n",
    "# image_paths = ['/well/papiez/users/hri611/python/foundation-models-radiology/pnx/017Negative/0c7c4bcaed8045da28007d3f5ad951b4/f9e703569f6b7cb2aecd350011688045/1fe308bc6e36a342e2a3693f37729c44/2490b1fb93d7c4700058df540fd60532.dcm','/well/papiez/users/hri611/python/foundation-models-radiology/pnx/017Positive/0d5bb46d8471b59a17517f7dfc22267d/95b02f3d3b4e867c40ec7e29dde6368e/d5b1bbc1340808d610ec61d97b4ddd4c/25a630ebeedc7f250ca2cc3e4693612d.dcm']\n",
    "image_paths = dicom_paths\n",
    "list_of_elixrb_embeddings = []\n",
    "\n",
    "for image_path in tqdm.tqdm(image_paths, desc=\"Processing images\"):\n",
    "  serialized_dicom_tf_example = dicom_to_tfexample(image_path).SerializeToString()\n",
    "\n",
    "  # Step 1 - ELIXR C (image to elixr C embeddings)\n",
    "\n",
    "  elixrc_output = elixrc_infer(input_example=tf.constant([serialized_dicom_tf_example]))\n",
    "  elixrc_embedding = elixrc_output['feature_maps_0'].numpy()\n",
    "\n",
    "  # Step 2 - Invoke QFormer with Elixr-C embeddings\n",
    "  # Initialize text inputs with zeros\n",
    "  qformer_input = {\n",
    "      'image_feature': elixrc_embedding.tolist(),\n",
    "      'ids': np.zeros((1, 1, 128), dtype=np.int32).tolist(),\n",
    "      'paddings':np.zeros((1, 1, 128), dtype=np.float32).tolist(),\n",
    "  }\n",
    "\n",
    "  qformer_output = qformer_model.signatures['serving_default'](**qformer_input)\n",
    "  elixrb_embeddings = qformer_output['all_contrastive_img_emb']\n",
    "\n",
    "  list_of_elixrb_embeddings.append(elixrb_embeddings)\n",
    "\n",
    "image_embeddings_df = pd.DataFrame()\n",
    "image_embeddings_df['image_paths'] = image_paths\n",
    "image_embeddings_df['elixrb_embeddings'] = list_of_elixrb_embeddings\n",
    "# image_embeddings_df.to_csv('image_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b828a543778f4cb7b1cfdbadadeba659",
      "4fb192d532bc49cea3e15dd79e08c081",
      "a54fc9266de84d8184b0fe528c13e5d3",
      "e3ff7768067745f7ac65fac2afc80026",
      "5c7bb787c224449fb6b2959db16782f6",
      "c2f6a4a1e2134fb2a31d80bb5a74ea2c",
      "0a3300cf791d49cc8bb538c56be5f1a0",
      "8d1612961f26422e9cb13282f7c7f5ce",
      "63517d4f2db54227bdf8fd40ee93ad5d",
      "a77c17478ccb4fa48cc0809e76f6804d",
      "c0da9415e8604a11b2d78f16843cc66e"
     ]
    },
    "id": "NShMhUub-830",
    "outputId": "efd9fa19-25cc-4952-8141-a4277b183daa"
   },
   "outputs": [],
   "source": [
    "# @title Generate text embeddings\n",
    "snapshot_download(repo_id=\"google/cxr-foundation\",local_dir=os.path.join(root_dir,'hf'),\n",
    "                  allow_patterns=['elixr-c-v2-pooled/*', 'pax-elixr-b-text/*'])\n",
    "\n",
    "# Run QFormer with text only.\n",
    "# Initialize image input with zeros\n",
    "tokens, paddings = bert_tokenize(\"No pneumothorax\")\n",
    "qformer_input = {\n",
    "    'image_feature': np.zeros([1, 8, 8, 1376], dtype=np.float32).tolist(),\n",
    "    'ids': tokens.tolist(),\n",
    "    'paddings': paddings.tolist(),\n",
    "}\n",
    "\n",
    "if 'qformer_model' not in locals():\n",
    "  qformer_model = tf.saved_model.load(os.path.join('hf/pax-elixr-b-text'))\n",
    "\n",
    "qformer_output = qformer_model.signatures['serving_default'](**qformer_input)\n",
    "negative_text_embedding = qformer_output['contrastive_txt_emb']\n",
    "\n",
    "# tokens, paddings = bert_tokenize(\"Small pneumothorax\")\n",
    "tokens, paddings = bert_tokenize(\"Pneumothorax\")\n",
    "\n",
    "qformer_input = {\n",
    "    'image_feature': np.zeros([1, 8, 8, 1376], dtype=np.float32).tolist(),\n",
    "    'ids': tokens.tolist(),\n",
    "    'paddings': paddings.tolist(),\n",
    "}\n",
    "\n",
    "if 'qformer_model' not in locals():\n",
    "  qformer_model = tf.saved_model.load(\"/content/hf/pax-elixr-b-text\")\n",
    "\n",
    "qformer_output = qformer_model.signatures['serving_default'](**qformer_input)\n",
    "positive_text_embedding = qformer_output['contrastive_txt_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7oyT69N-4JT"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions to get similarity score\n",
    "\n",
    "def compute_image_text_similarity(image_emb, txt_emb):\n",
    "  image_emb = np.reshape(image_emb, (32, 128))\n",
    "  similarities = []\n",
    "  for i in range(32):\n",
    "    # cosine similarity\n",
    "    similarity = np.dot(image_emb[i], txt_emb)/(np.linalg.norm(image_emb[i]) * np.linalg.norm(txt_emb))\n",
    "    similarities.append(similarity)\n",
    "  np_sm_similarities = np.array((similarities))\n",
    "  return np.max(np_sm_similarities)\n",
    "\n",
    "def zero_shot(image_emb, pos_txt_emb,neg_txt_emb):\n",
    "  pos_cosine = compute_image_text_similarity(image_emb, pos_txt_emb)\n",
    "  neg_cosine = compute_image_text_similarity(image_emb, neg_txt_emb)\n",
    "  return pos_cosine - neg_cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5WVffPSBn7D"
   },
   "outputs": [],
   "source": [
    "# Iterate over each image_id in eval_data_df\n",
    "for index, row in image_embeddings_df.iterrows():\n",
    "  image_id = row['image_paths']\n",
    "  # Get the embedding for the current image_id from image_embeddings_df\n",
    "  image_embedding = image_embeddings_df[image_embeddings_df['image_paths'] == image_id]['elixrb_embeddings'].iloc[0]\n",
    "  # Compute the similarity using the zero_shot function\n",
    "  similarity_score = zero_shot(image_embedding, positive_text_embedding[0], negative_text_embedding[0])\n",
    "  # Store the similarity score in a new column named 'score'\n",
    "  image_embeddings_df.loc[index, 'score'] = similarity_score\n",
    "\n",
    "# image_embeddings_df.to_csv('ptx_image_embeddings_with_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "final_df['image_paths'] = image_embeddings_df['image_paths'].apply(lambda x: x.replace((str(root_dir)+'/PTX Head to Head Study Data/'), ''))\n",
    "final_df['score'] = image_embeddings_df['score']/2+0.5\n",
    "final_df['binary_score'] = final_df['score'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "print(len(final_df[final_df['binary_score'] == 1]), len(final_df[final_df['binary_score'] == 0]))\n",
    "plt.hist(final_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('ptx_google_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_df = pd.read_csv('ptx_biomedclip_scores.csv')\n",
    "google_df = pd.read_csv('ptx_google_scores.csv')\n",
    "\n",
    "merged_df = pd.merge(microsoft_df, google_df, on='image_paths', suffixes=('_microsoft', '_google'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('ptx_google_micrsoft_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['binary_score_google','binary_score_microsoft']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['binary_score_google']==1&(merged_df['binary_score_microsoft']==1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(merged_df['binary_score_microsoft'], merged_df['binary_score_google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(merged_df['score_microsoft'], merged_df['score_google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
