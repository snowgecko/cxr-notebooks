{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "IwniTnZarRV6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def safe_pip_install(package):\n",
    "    try:\n",
    "        # Detect Colab\n",
    "        in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "        # Use shell-style install in Colab\n",
    "        if in_colab:\n",
    "            print(f\"Installing {package} in Colab...\")\n",
    "            # Use !pip or %pip to avoid subprocess errors\n",
    "            get_ipython().system(f\"pip install {package}\")\n",
    "        else:\n",
    "            print(f\"Installing {package} in standard environment...\")\n",
    "            # Use subprocess for non-Colab environments\n",
    "            import subprocess\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to install {package}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54eiwvswqe4P",
    "outputId": "55b4755f-a877-407f-fef5-e27c4d4cb336"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def detect_environment():\n",
    "    # Check for GPU\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "\n",
    "    # Check for Colab-specific environment\n",
    "    is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "    # Check for RunPod-specific environment\n",
    "    is_runpod = 'RUNPOD_POD_ID' in os.environ or os.path.exists('/workspace')\n",
    "\n",
    "    if is_runpod and has_gpu:\n",
    "        return 'runpod'\n",
    "    elif is_colab and not has_gpu:\n",
    "        return 'colab'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "#check the environement\n",
    "env = detect_environment()\n",
    "\n",
    "if env == 'runpod':\n",
    "    print(\"Running on RunPod with GPU — using full model.\")\n",
    "    # Load full CheXagent, enable CUDA, etc.\n",
    "    #GPU update:\n",
    "    safe_pip_install(\"transformers==4.40.0\")\n",
    "    safe_pip_install(\"torch\")\n",
    "    safe_pip_install(\"torchvision\")\n",
    "    safe_pip_install(\"pydicom\")\n",
    "    safe_pip_install(\"opencv-python\")\n",
    "    safe_pip_install(\"Pillow\")\n",
    "    safe_pip_install(\"accelerate\")\n",
    "elif env == 'colab':\n",
    "    print(\"Running on Colab with CPU — using lightweight fallback.\")\n",
    "    safe_pip_install(\"transformers==4.40.0\")\n",
    "    safe_pip_install(\"transformers==4.40.0\")\n",
    "    safe_pip_install(\"torch\")\n",
    "    safe_pip_install(\"torchvision\")\n",
    "    safe_pip_install(\"pydicom\")\n",
    "    safe_pip_install(\"opencv-python\")\n",
    "    safe_pip_install(\"Pillow\")\n",
    "    safe_pip_install(\"accelerate\")\n",
    "else:\n",
    "    print(\"Unknown environment — defaulting to safe config.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "0",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#Imports\n",
    "import os, glob\n",
    "import torch\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "6ed51099259b431291aeb95fa3cd04da",
      "11f4e03a22f14ee68c8f79bc726b0d50",
      "f410ef82d9a44e74aa968b80588fdb32",
      "a36503ab3590418fa801905e74f176c4",
      "97571583018440b4ba177837543283ca",
      "066a7f1c38ba48aebc51f41fc41ba2a8",
      "ff754dd744994f27abe4fcb0429ce656",
      "8ee401ec98754ab0b1551a378882e180",
      "726304a6e595424a8310e01395f5a9da",
      "8f85680b4f234235954f9e247c1722d0",
      "3972137581a44bb5848d46d723ccb7b6"
     ]
    },
    "id": "Oclhf1GmOKxB",
    "outputId": "d4e578ad-d3b7-4c0a-eaa6-6875d7f83a3d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"StanfordAIMI/CheXagent-2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "##device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "###########seems to thin that calling the if torch.cudo.is_available() as a param should work!!\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define query\n",
    "question = \"Does this chest X-ray show a pneumothorax?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUCGwVYmO9lU",
    "outputId": "fbc8d08f-f261-4237-f692-18102be7dcdd"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.device.type == \"meta\":\n",
    "        print(f\"{name} is on the meta device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLMwuflKNJzM",
    "outputId": "0bf354cb-7bf7-4dcd-f033-bc87f40d3abf"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#check the environement\n",
    "env = detect_environment()\n",
    "if env == 'runpod':\n",
    "  ##rclone sync gdrive:/MyDrive/MLProjects/foundation-models-radiology /workspace/MLProjects/foundation-models-radiology\n",
    "  ROOT = Path('/workspace/MLProjects/foundation-models-radiology')\n",
    "elif env == 'colab':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  ###once mounted the folders can be referenced\n",
    "  ROOT = Path('/content/drive/MyDrive/MLProjects/foundation-models-radiology')\n",
    "\n",
    "else:\n",
    "  sys.exit(\"Error: No platform recognised\")\n",
    "\n",
    "DICOM_DIR = ROOT / 'PTXHeadtoHeadSmall'   # use the exact folder name as on Drive\n",
    "JPEG_DIR = ROOT / 'cxr_jpegs'\n",
    "JPEG_DIR.mkdir(exist_ok=True)\n",
    "print(\"exists:\", ROOT.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ooUdS6GHTjb",
    "outputId": "1ee993a2-33f0-4566-d674-273710acb90e"
   },
   "outputs": [],
   "source": [
    "#Set paths\n",
    "\n",
    "#ROOT = Path(\"/content/drive/MyDrive\")  # adjust if needed\n",
    "#DICOM_DIR = ROOT / \"PTXHeadtoHeadSmall\"\n",
    "\n",
    "#Replace with: direct file access or use rclone to sync your Google Drive into the pod’s local filesystem.\n",
    "#RunPod doesn’t support drive.mount().\n",
    "#GPU update:\n",
    "#ROOT = Path('/workspace/MLProjects/foundation-models-radiology')\n",
    "#JPEG_DIR = ROOT / 'cxr_jpegs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvPGzs3iHV4j",
    "outputId": "04639e1a-c244-4b2f-f2d6-d7042c3d9433"
   },
   "outputs": [],
   "source": [
    "# Convert DICOMs to JPEGs\n",
    "def dicom_to_jpeg(dicom_path, jpeg_path):\n",
    "    ds = pydicom.dcmread(str(dicom_path))\n",
    "    img = ds.pixel_array\n",
    "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    img = cv2.equalizeHist(img)  # optional contrast enhancement\n",
    "    cv2.imwrite(str(jpeg_path), img)\n",
    "\n",
    "# Batch convert\n",
    "#dicom_paths = list(DICOM_DIR.rglob(\"*.dcm\"))\n",
    "# find DICOMs (case-insensitive .dcm)\n",
    "dicom_paths = list(DICOM_DIR.rglob('*.[dD][cC][mM]'))\n",
    "print(\"count:\", len(dicom_paths))\n",
    "print(\"samples:\", dicom_paths[:5])\n",
    "\n",
    "jpeg_paths = []\n",
    "for dcm_path in dicom_paths:\n",
    "    jpg_path = JPEG_DIR / f\"{dcm_path.stem}.jpg\"\n",
    "    dicom_to_jpeg(dcm_path, jpg_path)\n",
    "    jpeg_paths.append(str(jpg_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "pqQTTwQZHeaY"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def ask_chexagent(image_path, question):\n",
    "    try:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        query = tokenizer.from_list_format([\n",
    "            {\"image\": str(image_path)},\n",
    "            {\"text\": question}\n",
    "        ])\n",
    "        conversation = [\n",
    "            {\"from\": \"system\", \"value\": \"You are a helpful assistant.\"},\n",
    "            {\"from\": \"human\",  \"value\": query}\n",
    "        ]\n",
    "        # Returns a tensor, not a dict\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            conversation, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        # Attention mask (optional; model can infer it, but this is safe)\n",
    "        attention_mask = input_ids.ne(tokenizer.pad_token_id) if tokenizer.pad_token_id is not None else None\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # ← this line\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            use_cache=True,\n",
    "            max_new_tokens=512\n",
    "        )[0]\n",
    "\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Error processing {image_path}: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZq4ckQ4Kqor",
    "outputId": "94cb8acd-ac4b-421e-f5b6-5ea3164b2a78"
   },
   "outputs": [],
   "source": [
    "# Run on all images and collect responses\n",
    "import csv\n",
    "\n",
    "results = []\n",
    "for path in jpeg_paths:\n",
    "    answer = ask_chexagent(path, question)\n",
    "    results.append((path, answer))\n",
    "    print(f\" {Path(path).name} → {answer}\")\n",
    "\n",
    "with open(\"chexagent_results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Image\", \"Answer\"])\n",
    "    writer.writerows(results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
