{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.0\n",
        "!pip install -q torch torchvision pydicom opencv-python Pillow accelerate\n",
        "\n",
        "#GPU update: pip install torch torchvision transformers==4.40.0 pydicom opencv-python Pillow accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kts_kbpaIH08",
        "outputId": "c22eff45-f998-47de-b0ed-ac1a6ee6e396"
      },
      "id": "kts_kbpaIH08",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.12/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.0) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "0"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "#Imports\n",
        "import os, glob\n",
        "import torch\n",
        "import pydicom\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"StanfordAIMI/CheXagent-2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "##device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "###########seems to thin that calling the if torch.cudo.is_available() as a param should work!!\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define query\n",
        "question = \"Does this chest X-ray show a pneumothorax?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "5a850bc4b3184917bd4fab99f6777edf",
            "a5ce1a0407da4312a7f760eafc2780c4",
            "9114c7cf9f8d4c469b9318e13f4ca60b",
            "6d1aea60c3f347609f33d3750c8ecff9",
            "a391257c126d4d26adcaad77a315fb54",
            "ab36da77d0954f2d948d2f16912d3ae5",
            "8bd8f0d12f49466d9c9a20059a7b5099",
            "944716957c864258bae807aae3386e5d",
            "6266cd2eb3714cfabd6854ce205f0168",
            "193d75d1989c45948b6d380b01e39562",
            "65e0a574aeaa472d9cef803b11a52084"
          ]
        },
        "id": "Oclhf1GmOKxB",
        "outputId": "1af03284-2a8b-4c41-c41c-39cda3b3bdae"
      },
      "id": "Oclhf1GmOKxB",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a850bc4b3184917bd4fab99f6777edf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.device.type == \"meta\":\n",
        "        print(f\"{name} is on the meta device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uUCGwVYmO9lU",
        "outputId": "d750df67-d04d-427e-8e0b-3c9f2a5d1ccb"
      },
      "id": "uUCGwVYmO9lU",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.layers.28.self_attn.q_proj.weight is on the meta device\n",
            "model.layers.28.self_attn.q_proj.bias is on the meta device\n",
            "model.layers.28.self_attn.k_proj.weight is on the meta device\n",
            "model.layers.28.self_attn.k_proj.bias is on the meta device\n",
            "model.layers.28.self_attn.v_proj.weight is on the meta device\n",
            "model.layers.28.self_attn.v_proj.bias is on the meta device\n",
            "model.layers.28.self_attn.dense.weight is on the meta device\n",
            "model.layers.28.self_attn.dense.bias is on the meta device\n",
            "model.layers.28.mlp.fc1.weight is on the meta device\n",
            "model.layers.28.mlp.fc1.bias is on the meta device\n",
            "model.layers.28.mlp.fc2.weight is on the meta device\n",
            "model.layers.28.mlp.fc2.bias is on the meta device\n",
            "model.layers.28.input_layernorm.weight is on the meta device\n",
            "model.layers.28.input_layernorm.bias is on the meta device\n",
            "model.layers.29.self_attn.q_proj.weight is on the meta device\n",
            "model.layers.29.self_attn.q_proj.bias is on the meta device\n",
            "model.layers.29.self_attn.k_proj.weight is on the meta device\n",
            "model.layers.29.self_attn.k_proj.bias is on the meta device\n",
            "model.layers.29.self_attn.v_proj.weight is on the meta device\n",
            "model.layers.29.self_attn.v_proj.bias is on the meta device\n",
            "model.layers.29.self_attn.dense.weight is on the meta device\n",
            "model.layers.29.self_attn.dense.bias is on the meta device\n",
            "model.layers.29.mlp.fc1.weight is on the meta device\n",
            "model.layers.29.mlp.fc1.bias is on the meta device\n",
            "model.layers.29.mlp.fc2.weight is on the meta device\n",
            "model.layers.29.mlp.fc2.bias is on the meta device\n",
            "model.layers.29.input_layernorm.weight is on the meta device\n",
            "model.layers.29.input_layernorm.bias is on the meta device\n",
            "model.layers.30.self_attn.q_proj.weight is on the meta device\n",
            "model.layers.30.self_attn.q_proj.bias is on the meta device\n",
            "model.layers.30.self_attn.k_proj.weight is on the meta device\n",
            "model.layers.30.self_attn.k_proj.bias is on the meta device\n",
            "model.layers.30.self_attn.v_proj.weight is on the meta device\n",
            "model.layers.30.self_attn.v_proj.bias is on the meta device\n",
            "model.layers.30.self_attn.dense.weight is on the meta device\n",
            "model.layers.30.self_attn.dense.bias is on the meta device\n",
            "model.layers.30.mlp.fc1.weight is on the meta device\n",
            "model.layers.30.mlp.fc1.bias is on the meta device\n",
            "model.layers.30.mlp.fc2.weight is on the meta device\n",
            "model.layers.30.mlp.fc2.bias is on the meta device\n",
            "model.layers.30.input_layernorm.weight is on the meta device\n",
            "model.layers.30.input_layernorm.bias is on the meta device\n",
            "model.layers.31.self_attn.q_proj.weight is on the meta device\n",
            "model.layers.31.self_attn.q_proj.bias is on the meta device\n",
            "model.layers.31.self_attn.k_proj.weight is on the meta device\n",
            "model.layers.31.self_attn.k_proj.bias is on the meta device\n",
            "model.layers.31.self_attn.v_proj.weight is on the meta device\n",
            "model.layers.31.self_attn.v_proj.bias is on the meta device\n",
            "model.layers.31.self_attn.dense.weight is on the meta device\n",
            "model.layers.31.self_attn.dense.bias is on the meta device\n",
            "model.layers.31.mlp.fc1.weight is on the meta device\n",
            "model.layers.31.mlp.fc1.bias is on the meta device\n",
            "model.layers.31.mlp.fc2.weight is on the meta device\n",
            "model.layers.31.mlp.fc2.bias is on the meta device\n",
            "model.layers.31.input_layernorm.weight is on the meta device\n",
            "model.layers.31.input_layernorm.bias is on the meta device\n",
            "model.final_layernorm.weight is on the meta device\n",
            "model.final_layernorm.bias is on the meta device\n",
            "model.visual.pos_embed is on the meta device\n",
            "model.visual.proj is on the meta device\n",
            "model.visual.model.embeddings.patch_embedding.weight is on the meta device\n",
            "model.visual.model.embeddings.patch_embedding.bias is on the meta device\n",
            "model.visual.model.embeddings.position_embedding.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.0.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.0.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.1.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.1.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.2.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.2.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.3.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.3.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.4.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.4.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.5.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.5.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.6.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.6.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.7.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.7.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.8.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.8.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.9.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.9.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.10.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.10.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.11.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.11.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.12.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.12.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.13.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.13.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.14.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.14.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.15.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.15.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.16.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.16.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.17.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.17.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.18.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.18.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.19.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.19.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.20.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.20.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.21.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.21.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.22.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.22.layer_norm2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.k_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.k_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.v_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.v_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.q_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.q_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.out_proj.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.self_attn.out_proj.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.layer_norm1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.layer_norm1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.mlp.fc2.bias is on the meta device\n",
            "model.visual.model.encoder.layers.23.layer_norm2.weight is on the meta device\n",
            "model.visual.model.encoder.layers.23.layer_norm2.bias is on the meta device\n",
            "model.visual.model.post_layernorm.weight is on the meta device\n",
            "model.visual.model.post_layernorm.bias is on the meta device\n",
            "model.visual.model.head.probe is on the meta device\n",
            "model.visual.model.head.attention.in_proj_weight is on the meta device\n",
            "model.visual.model.head.attention.in_proj_bias is on the meta device\n",
            "model.visual.model.head.attention.out_proj.weight is on the meta device\n",
            "model.visual.model.head.attention.out_proj.bias is on the meta device\n",
            "model.visual.model.head.layernorm.weight is on the meta device\n",
            "model.visual.model.head.layernorm.bias is on the meta device\n",
            "model.visual.model.head.mlp.fc1.weight is on the meta device\n",
            "model.visual.model.head.mlp.fc1.bias is on the meta device\n",
            "model.visual.model.head.mlp.fc2.weight is on the meta device\n",
            "model.visual.model.head.mlp.fc2.bias is on the meta device\n",
            "model.visual.attn_pool.0.weight is on the meta device\n",
            "model.visual.attn_pool.0.bias is on the meta device\n",
            "model.visual.attn_pool.2.weight is on the meta device\n",
            "model.visual.attn_pool.2.bias is on the meta device\n",
            "model.visual.ln_post.weight is on the meta device\n",
            "model.visual.ln_post.bias is on the meta device\n",
            "lm_head.weight is on the meta device\n",
            "lm_head.bias is on the meta device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attach the colab project to teh files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLMwuflKNJzM",
        "outputId": "4706b8eb-9577-4d72-c563-0c5166ff0a8b"
      },
      "id": "zLMwuflKNJzM",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set paths\n",
        "from pathlib import Path\n",
        "ROOT = Path('/content/drive/MyDrive/MLProjects/foundation-models-radiology')\n",
        "DICOM_DIR = ROOT / 'PTXHeadtoHeadSmall'   # use the exact folder name as on Drive\n",
        "print(\"exists:\", ROOT.exists())\n",
        "\n",
        "#ROOT = Path(\"/content/drive/MyDrive\")  # adjust if needed\n",
        "#DICOM_DIR = ROOT / \"PTXHeadtoHeadSmall\"\n",
        "JPEG_DIR = Path(\"/content/drive/MyDrive/MLProjects/foundation-models-radiology/cxr_jpegs\")\n",
        "JPEG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "#Replace with: direct file access or use rclone to sync your Google Drive into the pods local filesystem.\n",
        "#RunPod doesnt support drive.mount().\n",
        "#GPU update:\n",
        "#ROOT = Path('/workspace/MLProjects/foundation-models-radiology')\n",
        "#JPEG_DIR = ROOT / 'cxr_jpegs'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ooUdS6GHTjb",
        "outputId": "1ee993a2-33f0-4566-d674-273710acb90e"
      },
      "id": "_ooUdS6GHTjb",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DICOMs to JPEGs\n",
        "def dicom_to_jpeg(dicom_path, jpeg_path):\n",
        "    ds = pydicom.dcmread(str(dicom_path))\n",
        "    img = ds.pixel_array\n",
        "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "    img = cv2.equalizeHist(img)  # optional contrast enhancement\n",
        "    cv2.imwrite(str(jpeg_path), img)\n",
        "\n",
        "# Batch convert\n",
        "#dicom_paths = list(DICOM_DIR.rglob(\"*.dcm\"))\n",
        "# find DICOMs (case-insensitive .dcm)\n",
        "dicom_paths = list(DICOM_DIR.rglob('*.[dD][cC][mM]'))\n",
        "print(\"count:\", len(dicom_paths))\n",
        "print(\"samples:\", dicom_paths[:5])\n",
        "\n",
        "jpeg_paths = []\n",
        "for dcm_path in dicom_paths:\n",
        "    jpg_path = JPEG_DIR / f\"{dcm_path.stem}.jpg\"\n",
        "    dicom_to_jpeg(dcm_path, jpg_path)\n",
        "    jpeg_paths.append(str(jpg_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvPGzs3iHV4j",
        "outputId": "ffb71804-7562-4f94-f7b2-a3aedb64ba6c"
      },
      "id": "AvPGzs3iHV4j",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count: 2\n",
            "samples: [PosixPath('/content/drive/MyDrive/MLProjects/foundation-models-radiology/PTXHeadtoHeadSmall/0a1d53cc79cb2ee50afa507d425f6d15/04797ae93ba9e94174bd928999395887/204611bfbd3e2c990bc2e9d6e131d46a/813f8b5f7031f381d6aadb2edcdd15cf.dcm'), PosixPath('/content/drive/MyDrive/MLProjects/foundation-models-radiology/PTXHeadtoHeadSmall/ffac9214400053aa4c78126255328235/113cd0ccb7393ecb0a5a8a94dac38f72/73cedef0efa5a294616db1be804bd026/3dc870d5ffa18cb81c81342ca478eb4f.dcm')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on each image\n",
        "def ask_chexagent(image_path, question):\n",
        "    query = tokenizer.from_list_format([\n",
        "        {\"image\": image_path},\n",
        "        {\"text\": question}\n",
        "    ])\n",
        "    conversation = [\n",
        "        {\"from\": \"system\", \"value\": \"You are a helpful assistant.\"},\n",
        "        {\"from\": \"human\", \"value\": query}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    #This returns a dictionary with: input_ids: tokenized input attention_mask: mask for padding\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        temperature=1.0,\n",
        "        top_p=1.0,\n",
        "        use_cache=True,\n",
        "        max_new_tokens=128\n",
        "    )[0]\n",
        "\n",
        "    response = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    #response = tokenizer.decode(output[input_ids.size(1):-1])\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "pqQTTwQZHeaY"
      },
      "id": "pqQTTwQZHeaY",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on all images and collect responses\n",
        "import csv\n",
        "\n",
        "results = []\n",
        "for path in jpeg_paths:\n",
        "    answer = ask_chexagent(path, question)\n",
        "    results.append((path, answer))\n",
        "    print(f\" {Path(path).name}  {answer}\")\n",
        "\n",
        "with open(\"chexagent_results.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Image\", \"Answer\"])\n",
        "    writer.writerows(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "xZq4ckQ4Kqor",
        "outputId": "fdbcdda5-10fa-43d7-c897-7fd9e8eda6f6"
      },
      "id": "xZq4ckQ4Kqor",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Cannot copy out of meta tensor; no data!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4218043668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjpeg_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_chexagent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" {Path(path).name}  {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3505248331.py\u001b[0m in \u001b[0;36mask_chexagent\u001b[0;34m(image_path, question)\u001b[0m\n\u001b[1;32m     15\u001b[0m     ).to(device)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m             result = self._greedy_search(\n\u001b[0m\u001b[1;32m   1577\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2495\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/StanfordAIMI/CheXagent-2-3b/463999422d77fe01380ef03493e5ae3bb11bd004/modeling_chexagent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/StanfordAIMI/CheXagent-2-3b/463999422d77fe01380ef03493e5ae3bb11bd004/modeling_chexagent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_pad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mimage_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/StanfordAIMI/CheXagent-2-3b/463999422d77fe01380ef03493e5ae3bb11bd004/modeling_visual.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, image_paths, training)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 )\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         return send_to_device(args, self.execution_device), send_to_device(\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         return honor_type(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         return honor_type(\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         )\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"npu:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a850bc4b3184917bd4fab99f6777edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5ce1a0407da4312a7f760eafc2780c4",
              "IPY_MODEL_9114c7cf9f8d4c469b9318e13f4ca60b",
              "IPY_MODEL_6d1aea60c3f347609f33d3750c8ecff9"
            ],
            "layout": "IPY_MODEL_a391257c126d4d26adcaad77a315fb54"
          }
        },
        "a5ce1a0407da4312a7f760eafc2780c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab36da77d0954f2d948d2f16912d3ae5",
            "placeholder": "",
            "style": "IPY_MODEL_8bd8f0d12f49466d9c9a20059a7b5099",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "9114c7cf9f8d4c469b9318e13f4ca60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_944716957c864258bae807aae3386e5d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6266cd2eb3714cfabd6854ce205f0168",
            "value": 3
          }
        },
        "6d1aea60c3f347609f33d3750c8ecff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193d75d1989c45948b6d380b01e39562",
            "placeholder": "",
            "style": "IPY_MODEL_65e0a574aeaa472d9cef803b11a52084",
            "value": "3/3[00:02&lt;00:00,1.21s/it]"
          }
        },
        "a391257c126d4d26adcaad77a315fb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab36da77d0954f2d948d2f16912d3ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd8f0d12f49466d9c9a20059a7b5099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "944716957c864258bae807aae3386e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6266cd2eb3714cfabd6854ce205f0168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "193d75d1989c45948b6d380b01e39562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e0a574aeaa472d9cef803b11a52084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}