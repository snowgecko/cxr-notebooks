{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kts_kbpaIH08",
    "outputId": "c22eff45-f998-47de-b0ed-ac1a6ee6e396"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.0\n",
    "!pip install -q torch torchvision pydicom opencv-python Pillow accelerate\n",
    "\n",
    "#GPU update: pip install torch torchvision transformers==4.40.0 pydicom opencv-python Pillow accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "0",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#Imports\n",
    "import os, glob\n",
    "import torch\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "5a850bc4b3184917bd4fab99f6777edf",
      "a5ce1a0407da4312a7f760eafc2780c4",
      "9114c7cf9f8d4c469b9318e13f4ca60b",
      "6d1aea60c3f347609f33d3750c8ecff9",
      "a391257c126d4d26adcaad77a315fb54",
      "ab36da77d0954f2d948d2f16912d3ae5",
      "8bd8f0d12f49466d9c9a20059a7b5099",
      "944716957c864258bae807aae3386e5d",
      "6266cd2eb3714cfabd6854ce205f0168",
      "193d75d1989c45948b6d380b01e39562",
      "65e0a574aeaa472d9cef803b11a52084"
     ]
    },
    "id": "Oclhf1GmOKxB",
    "outputId": "1af03284-2a8b-4c41-c41c-39cda3b3bdae"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"StanfordAIMI/CheXagent-2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "##device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "###########seems to thin that calling the if torch.cudo.is_available() as a param should work!!\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define query\n",
    "question = \"Does this chest X-ray show a pneumothorax?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUCGwVYmO9lU",
    "outputId": "d750df67-d04d-427e-8e0b-3c9f2a5d1ccb"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.device.type == \"meta\":\n",
    "        print(f\"{name} is on the meta device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLMwuflKNJzM",
    "outputId": "4706b8eb-9577-4d72-c563-0c5166ff0a8b"
   },
   "outputs": [],
   "source": [
    "# attach the colab project to teh files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ooUdS6GHTjb",
    "outputId": "1ee993a2-33f0-4566-d674-273710acb90e"
   },
   "outputs": [],
   "source": [
    "#Set paths\n",
    "from pathlib import Path\n",
    "ROOT = Path('/content/drive/MyDrive/MLProjects/foundation-models-radiology')\n",
    "DICOM_DIR = ROOT / 'PTXHeadtoHeadSmall'   # use the exact folder name as on Drive\n",
    "print(\"exists:\", ROOT.exists())\n",
    "\n",
    "#ROOT = Path(\"/content/drive/MyDrive\")  # adjust if needed\n",
    "#DICOM_DIR = ROOT / \"PTXHeadtoHeadSmall\"\n",
    "JPEG_DIR = Path(\"/content/drive/MyDrive/MLProjects/foundation-models-radiology/cxr_jpegs\")\n",
    "JPEG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#Replace with: direct file access or use rclone to sync your Google Drive into the pod’s local filesystem.\n",
    "#RunPod doesn’t support drive.mount().\n",
    "#GPU update:\n",
    "#ROOT = Path('/workspace/MLProjects/foundation-models-radiology')\n",
    "#JPEG_DIR = ROOT / 'cxr_jpegs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvPGzs3iHV4j",
    "outputId": "ffb71804-7562-4f94-f7b2-a3aedb64ba6c"
   },
   "outputs": [],
   "source": [
    "# Convert DICOMs to JPEGs\n",
    "def dicom_to_jpeg(dicom_path, jpeg_path):\n",
    "    ds = pydicom.dcmread(str(dicom_path))\n",
    "    img = ds.pixel_array\n",
    "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    img = cv2.equalizeHist(img)  # optional contrast enhancement\n",
    "    cv2.imwrite(str(jpeg_path), img)\n",
    "\n",
    "# Batch convert\n",
    "#dicom_paths = list(DICOM_DIR.rglob(\"*.dcm\"))\n",
    "# find DICOMs (case-insensitive .dcm)\n",
    "dicom_paths = list(DICOM_DIR.rglob('*.[dD][cC][mM]'))\n",
    "print(\"count:\", len(dicom_paths))\n",
    "print(\"samples:\", dicom_paths[:5])\n",
    "\n",
    "jpeg_paths = []\n",
    "for dcm_path in dicom_paths:\n",
    "    jpg_path = JPEG_DIR / f\"{dcm_path.stem}.jpg\"\n",
    "    dicom_to_jpeg(dcm_path, jpg_path)\n",
    "    jpeg_paths.append(str(jpg_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "pqQTTwQZHeaY"
   },
   "outputs": [],
   "source": [
    "# Run inference on each image\n",
    "def ask_chexagent(image_path, question):\n",
    "    query = tokenizer.from_list_format([\n",
    "        {\"image\": image_path},\n",
    "        {\"text\": question}\n",
    "    ])\n",
    "    conversation = [\n",
    "        {\"from\": \"system\", \"value\": \"You are a helpful assistant.\"},\n",
    "        {\"from\": \"human\", \"value\": query}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    #This returns a dictionary with: input_ids: tokenized input attention_mask: mask for padding\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=128\n",
    "    )[0]\n",
    "\n",
    "    response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    #response = tokenizer.decode(output[input_ids.size(1):-1])\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "xZq4ckQ4Kqor",
    "outputId": "fdbcdda5-10fa-43d7-c897-7fd9e8eda6f6"
   },
   "outputs": [],
   "source": [
    "# Run on all images and collect responses\n",
    "import csv\n",
    "\n",
    "results = []\n",
    "for path in jpeg_paths:\n",
    "    answer = ask_chexagent(path, question)\n",
    "    results.append((path, answer))\n",
    "    print(f\" {Path(path).name} → {answer}\")\n",
    "\n",
    "with open(\"chexagent_results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Image\", \"Answer\"])\n",
    "    writer.writerows(results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
