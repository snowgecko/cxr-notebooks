{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#####code check from CP#################\n",
    "!pip install -q torch torchvision transformers pydicom opencv-python Pillow accelerate\n",
    "# Imports\n",
    "import os, glob\n",
    "import torch\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer \n",
    "\n",
    "\n",
    "################GLOBAL PATHS\n",
    "ROOT = Path(\"/content/drive/MyDrive\")  \n",
    "DICOM_DIR = ROOT / \"PTXHeadtoHeadStudyData\"\n",
    "JPEG_DIR = Path(\"/content/cxr_jpegs\")\n",
    "JPEG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#####from the Hugging face page#################\n",
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# step 1: Setup constant\n",
    "model_name = \"StanfordAIMI/CheXagent-2-3b\"\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "\n",
    "# step 2: Load Processor and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "model = model.to(dtype)\n",
    "model.eval()\n",
    "\n",
    "# step 3: Inference\n",
    "query = tokenizer.from_list_format([*[{'image': path} for path in paths], {'text': prompt}])\n",
    "conv = [{\"from\": \"system\", \"value\": \"You are a helpful assistant.\"}, {\"from\": \"human\", \"value\": query}]\n",
    "input_ids = tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    input_ids.to(device), do_sample=False, num_beams=1, temperature=1., top_p=1., use_cache=True,\n",
    "    max_new_tokens=512\n",
    ")[0]\n",
    "response = tokenizer.decode(output[input_ids.size(1):-1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
